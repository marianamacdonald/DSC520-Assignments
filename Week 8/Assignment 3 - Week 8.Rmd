---
title: "Assignment 3 - Week 8"
output: pdf_document
date: '2022-05-11'
---

Set the working directory to the root of your DSC 520 directory
```{r}
setwd("/Users/marianamacdonald/Documents/DATA SCIENCE/DSC 520/Statistics R/Week 2/dsc520")
library(readxl)
real.estate <- read_excel("data/week-7-housing.xlsx")
```


### Complete the following: 
### 1) Explain any transformations or modifications you made to the dataset 

I have added a . (dot) for variables that were separated by a space (Sale.Price for example)

 
### 2) Create two variables; one that will contain the variables Sale Price and Square Foot of Lot (same variables used from previous assignment on simple regression) and one that will contain Sale Price and several additional predictors of your choice. Explain the basis for your additional predictor selections.


```{r}
spsqlot1 <- lm(Sale.Price ~ sq_ft_lot, data = real.estate)
spsqlot2 <- lm(Sale.Price ~ sq_ft_lot + bedrooms + bath_full_count, data = real.estate)
```


I chose bedrooms and bath as additional predictor, because I predict that sale price will be higher if the sq_ft_lot and the bed and bath counts are higher.


### 3) Execute a summary() function on two variables defined in the previous step to compare the model results. What are the R2 and Adjusted R2 statistics? Explain what these results tell you about the overall model. Did the inclusion of the additional predictors help explain any large variations found in Sale Price?

```{r}
options(scipen=999)
summary(spsqlot1)
summary(spsqlot2)
```


R2 or the coefficient of determination explains how much the predictors explain the output. The adjusted R2 determines if other variables contribute to the model. Also, indicates the loss of predictive power (shrinkage).

When only sq_ft_lot is used as a predictor, the R2 is .014 or 1.4%. This means that sq_ft_lot only accounts for 1.4% of sales price. When bed and bath are included as predictors, this value increases to 0.1127 or 11.27% of variance in price. The adjusted R2 gives some idea of how well the model generalizes. The difference on the second model is 0.1127 - 0.1125 = 0.0002 (about 0.02%). This is good because if the model were derivedfrom the population rather than a sample, it would account for approximately 0.02% less variance in the outcome.

From summary 2, estimate, we can see: st_ft_lot increases by 1 unit, price increases by 0.72 / tvalue = 12.22 add one extra bedroom, price increases by 68K / tvalue = 17.09 extra bath, price increases by $145K / tvalue= 26.88 (this has more influence on sales price than the other 2 predictors)


### 4) Considering the parameters of the multiple regression model you have created. What are the standardized betas for each parameter and what do the values indicate?


```{r}
sd(real.estate$Sale.Price)
sd(real.estate$sq_ft_lot)
sd(real.estate$bedrooms)
sd(real.estate$bath_full_count)

library(lm.beta)
lm.beta(spsqlot2)
```


sq_ft_lot (standardized B = .10): This value indicates that as sq_ft_lot increases one standard deviation (56,933), house sales price increases by .10 standard deviations. The standard deviation for sales price is 404,381) and so this constitutes a change of 40,438 in sales price (0.10 x 404381.1). Therefore, for every 56,933 sq_ft_lot, an extra 40,438 is added to sales price.

bedrooms (standardized B = .14): This value indicates that as bedroom number increases one standard deviation (0.87), house sales price increases by .14 standard deviations. The standard deviation for sales price is 404,381 and  so this constitutes a change of 56,613 at sales price (0.14 x 404,381). Therefore, for every 0.87 extra bedroom, an extra 56,613 is added to sales price.
  
full bath count (standardized B = .23): This value indicates that as full bath count number increases one standard deviation (0.65), house sales price increases by .23 standard deviations. The standard deviation for sales price is 404,381 and so this constitutes a change  of 93,007 at sales price (0.23 x 404,381). Therefore, for every 0.65 additional bath, an extra 93,007 is added to sales price.

adding a bathroom to a house, will increase the sales price more than having extra sq_ft_lot or bedrooms.


  
### 5)  Calculate the confidence intervals for the parameters in your model and explain what the results indicate.


```{r}
confint(spsqlot2)
```


only sq_ft_lot has tight confidence intervals (0.60 and 0.83), indicating that the estimates for the current model (sqsqlot1) are likely representative of the true population values. The interval for bath_full_count is wider (but still does not cross zero), indicating that the parameter for this variable is less representative, but nevertheledd significant.



### 6) Assess the improvement of the new model compared to your original model (simple regression model) by  testing whether this change is significant by performing an analysis of variance.

```{r}
anova(spsqlot1, spsqlot2)
```


We can say that model 2 (spsqlot2) significantly improved the fit of the model to the data compared to model 1 (spsqlot1), 
F = 712.43,p<.001



### 7) Perform casewise diagnostics to identify outliers and/or influential cases, storing each function's output in a dataframe assigned to a unique variable name.

```{r}
real.estate$residuals <- resid(spsqlot2)

real.estate$standardized.residuals<- rstandard(spsqlot2)
real.estate$studentized.residuals<-rstudent(spsqlot2)
real.estate$cooks.distance<- cooks.distance(spsqlot2)
real.estate$dfbeta<- dfbeta(spsqlot2)
real.estate$dffit<- dffits(spsqlot2)
real.estate$leverage<- hatvalues(spsqlot2)
real.estate$covariance.ratios<- covratio(spsqlot2)
real.estate

real.estate[, c('Sale.Price', 'sq_ft_lot', 'bedrooms', "bath_full_count", "residuals", 
                "standardized.residuals", "studentized.residuals", "cooks.distance",
                "dfbeta")]

```


### 8) Calculate the standardized residuals using the appropriate command, specifying those that are +-2,  storing the results of large residuals in a variable you create.

```{r}
real.estate$large.residual <- real.estate$standardized.residuals>2 | 
  real.estate$standardized.residuals < -2
```



### 9) Use the appropriate function to show the sum of large residuals.

```{r}
sum(real.estate$large.residual)
```

The sample has 12,865 rows, and 329 cases are TRUE, meaning that they have a residual more than 2 or less than -2.


### 10) Which specific variables have large residuals (only cases that evaluate as TRUE)?

```{r}
real.estate[real.estate$large.residual, c("Sale.Price", "sq_ft_lot", "bedrooms", 
                                          "bath_full_count","standardized.residuals")]
```

We have 329 out of 12,865 or 2.55%

### 11) Investigate further by calculating the leverage, cooks distance, and covariance ratios. Comment on all cases that are problematic.

```{r}
obs_large_resid <- real.estate[real.estate$large.residual, c("standardized.residuals", "leverage",
                                        "cooks.distance", "covariance.ratios")]
```

```{r echo=FALSE}
leverage_threshold <- (3 / 12865) * 3
obs_large_resid$large.residual <- obs_large_resid$leverage > leverage_threshold
sum(obs_large_resid$large.residual)
```

Based on my data, which has 12,865 rows, I got 82 problematic cases, meaning, that they are over the threshold of acceptable values for covariance ratio, which could have an impact on the outcome variable.




### 12) Perform the necessary calculations to assess the assumption of independence and state if the condition is met or not.

```{r}
library(car)
durbinWatsonTest(spsqlot2)
```


The condition has not been met because DW is 0.70 (less than 1) and p-value = 0. 
There is no autocorrelation.


### 13) Perform the necessary calculations to assess the assumption of no multicollinearity and state if the condition is met or not.

```{r}
library(car)
library(carData)
vif(spsqlot2)
1/vif(spsqlot2)

mean(vif(spsqlot2))
vif(spsqlot2)
```


There is no collinearity within the data since VIF values are below 10 and tolerance statistics are above 0.2).



### 14) Visually check the assumptions related to the residuals using the plot() and hist() functions.Summarize what each graph is informing you of and if any anomalies are present.


```{r}
plot(spsqlot2)
hist(real.estate$studentized.residuals)
```


The  first plot (residuals vs Fitted) shows a random pattern, which indicates that the assumptions of linearity, randomness and homoscedasticity have been met.The second plot (Normal Q-Q), shows a deviation from normality, skewed.The hist looks like a normal distribution. 

### 15) Overall, is this regression model unbiased? If an unbiased regression model, what does this tell us about the sample vs. the entire population model?

Yes, it is unbiased. It generalizes well. The model can be applied to the entire population.
